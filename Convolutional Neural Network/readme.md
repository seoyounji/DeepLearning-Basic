<<<<<<< HEAD

=======
CNN이란?
-----------------------------------------

CNN은 Convolutional Neural Network의 약자로 이름에서 알 수 있다시피 Convolution이라는 전처리 작업이 들어가는 Neural Network 모델이다.      
입력으로 들어오는 데이터의 특징을 추출하여 컴퓨터가 스스로 특징표현을 학습(feature representation learning)하는 구조이기 때문에 주로 이미지를 인식하는데 사용된다.(물론 음성 및 1차원 데이터에도 사용 가능하다)       
그 동안은 SIFT나 HOG 방식을 이용해 이미지를 인식하고 분류했는데 2012년 CNN이 등장한 뒤로 그 동안의 이미지 분류와는 비교도 할 수 없을 정도로 높은 성과를 내게 되었다.     
Convolution layer와 pooling layer를 복합적으로 구성하여 모델을 구성한다.            

그렇다면 왜 CNN을 쓰기 시작했을까?
-------------------------------------------

CNN이 등장하기 전 쓰이던 DNN 모델을 살펴보자.    
일반 DNN은 기본적으로 1차원 형태의 데이터를 사용한다.    
때문에 2차원 형태의 데이터인 이미지가 DNN에 입력으로 들어가게 되는 경우 이를 종이 자르듯 길게 잘라 한 줄 데이터로 만들어서 처리했는데 이 과정에서 이미지의 가장 중요한 정보인 공간/지역적 정보(spatial/topological information)이 사라지게 되기 때문에 결국 이미지 인식이 아닌 단순 숫자 연산이 되어버려 학습시간과 정확도가 떨어질 수 밖에 없었다.    
이런 문제점을 해결하기 위해 나온 것이 CNN이다.    
CNN은 2차원 데이터인 이미지를 1차원 데이터로 만드는 대신 2차원 데이터 그대로 받음으로써 공간/지역적 정보를 유지한 채 연산을 한다.    
이러한 CNN의 중요 포인트는 이미지 전체보다는 부분을 보는 것, 그리고 이미지의 한 픽셀과 주변 픽셀들의 연관성을 살리는 것이다.    
그렇다면 이미지 전체보다는 부분을 보는 이유는 무엇일까?     
새 이미지가 주어졌을 때 이것이 새인지 아닌지 판단할 수 있는 모델을 만들고 있다고 해보자.    
이 때는 새의 가장 큰 특징인 부리가 있는지 없는지 보는 것이 중요한 판단의 지표가 될 것이다.     
그렇기 때문에 전체 이미지를 보는 것보단 새의 부리 부분만 잘라서 보는 것이 정확도에 큰 차이가 없으면서 연산의 크기나 횟수 면에서 효율적일 것이다.    
`이렇게 이미지가 주어졌을 때 어떠한 특징이 있는지 파악하는 것을 가능하게 해주는 것이 바로 CNN이다.`    



인간의 뇌는 신호를 받은 뒤 그 신호를 처리해 다시 전달하는 세포인 뉴런의 집합체이다.    
뇌의 구조는 굉장히 복잡하지만 그걸 구성하고 있는 연결체인 뉴런의 구조는 생각보다 단순한 구조인데 이걸 기계로 만들어 보자는 생각에서 출발한 것이 ANN 인 것이다.        
아래는 간단하게 뉴런의 작동방식을 도식화한 그림이다.<br/>
<img src="images/1.jpg" title="뉴런 동작방식" alt="뉴런 동작방식"></img><br/>

이 그림을 보면 input으로 들어온 각 x 값에 weight인 w 값이 각각 곱해지고 더해진 뒤 bias 값을 추가로 더하고 나온 값을 Activation function에 통과시키면 최종적으로 output이 나온다는 걸 알 수 있다.    

뉴런을 여러개 모아본다면 대략 아래와 같은 구조가 된다.<br/><br/>
<img src="images/4.PNG" title="신경망 구조" alt="신경망 구조"></img><br/>

이는 가운데 hidden layer가 1개인 가장 단순한 Single Layer Network 구조로 이를 이용해 AND나 OR 문제 같은 선형방정식을 해결할 수 있었다.    
하지만 XOR 문제가 등장하며 더이상 Single Layer Network로는 이 문제를 해결할 수 없게 되었다.    
XOR 문제는 풀기 위해선 hidden layer가 2개 이상 쌓여진 Multi Layer Network (Deep Neural Network)가 필요했지만 Multi Layer를 어떻게 만들어야하는지, 어떻게 학습시켜야 하는지 알 수 없었기 때문에 머신러닝에 암흑기가 도래하게 된다.    
(여담이지만 딥러닝이라는 이름도 이 hidden layer가 2개 이상 deep 하게 계속 쌓이는 것에서 유래된 것이다.)    
하지만 후에 `비선형 활성화 함수(Non-linear Activation function)과 Backpropagation 알고리즘의 도입으로 Multi Layer Network를 만들고 학습시킬 수 있게 되었다.`       
이 Multi Layer Network, 즉 Deep Neural Network의 경우 컴퓨터가 스스로 분류 레이블을 만들어 내고 데이터를 구분짓는 과정을 반복하여 최적의 결과를 도출해내게 되고 이를 응용한 알고리즘이 바로 CNN, RNN인 것이다.        

그렇다면 활성화 함수(Activation function)이란?
-----------------------------------

활성화 함수란 위 그림에서 보다시피 노드에서 입력값을 처리한 뒤 출력하기 전 계산한 값을 변환하기 위한 함수이다.    
이 함수는 비선형 함수여야 하는데 그 이유는 선형함수를 사용할 시 Multi Layer를 만드는 의미가 없기 때문이다.     
예를 들어 선형함수 h(x) = cx를 활성화 함수로 사용하는 3-layer Neural Network가 있다고 가정해보자.    
해당 NN을 식으로 나타내면 y(x) = h(h(h(x)))가 되는데 이는 사실 c^3x와 같기 때문에 결국 선형함수 g(x) = c^3x를 활성화 함수로 사용하는 1-layer NN과 같은 결과를 내게 된다.       
즉 linear한 연산을 가지는 layer는 수십개를 쌓아도 선형 함수의 특징 때문에 결국 하나의 linear 연산으로 나타낼 수 있기에 활성화 함수는 비선형 함수로 사용해야 하는 것이다.    
초기엔 활성화 함수로 아래의 step function을 사용했다.<br/><br/>
<img src="images/21.JPG" title="계단 함수" alt="계단 함수"></img><br/>

이제 활성화 함수를 이용해 Multi Layer Network를 만들 수 있게 되었다.    
딥러닝이라고 부를 수 있는 네트워크 구조가 만들어지기 시작한 것이다.    
그렇다면 이 Multi Layer Neural Network 어떻게 학습시켜야 하는 걸까?

그래서 나온 Backpropagation
-----------------------------

역전파 알고리즘이라고도 불리는 backpropagation 알고리즘은 오늘날 ANN을 학습시키기 위한 일반적인 알고리즘 중 하나이다.       
이 알고리즘의 구조는 아주 단순한데, `학습 결과로 나와야하는 target 값과 실제 모델을 통과하며 나온 output 값이 얼마나 차이가 나는지 구한 후 그 오차값을 뒤의 노드들에게 알려주면서 이 오차값이 0에 가까워지도록 각 노드가 가지고 있는 변수들(weight나 bias)을 갱신`하는 것이다.      
그렇다면 각 weight나 bias를 어떻게, 얼마나 갱신해야 할까?      
이 문제들은 Chain Rule 법칙을 사용해 해결할 수 있었다.    
>>>>>>> 998ffb908d279333da5c2fae00adf45a83c50634
